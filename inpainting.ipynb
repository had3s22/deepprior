{
 "cells": [
  {
   "metadata": {
    "_uuid": "c41cf0d7-23f1-472e-8103-1d3ec0c6b4b9",
    "_cell_guid": "0aa2a71f-df2d-43dc-ad0c-30079f541165",
    "trusted": true
   },
   "cell_type": "markdown",
   "source": [
    "Code for **\"Inpainting\"** figures $6$, $8$ and 7 (top) from the main paper."
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "_uuid": "82affe13-8cd0-4733-8580-81053c00153b",
    "_cell_guid": "0b3adf18-0e40-4ef6-a12e-1b057c97fd9b",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# !rm -rf /kaggle/working/*\n",
    "# !git clone https://github.com/had3s22/deepprior.git\n",
    "# !mv deepprior/* ./"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'deepprior'...\r\n",
      "remote: Enumerating objects: 82, done.\u001B[K\r\n",
      "remote: Counting objects: 100% (82/82), done.\u001B[K\r\n",
      "remote: Compressing objects: 100% (66/66), done.\u001B[K\r\n",
      "remote: Total 82 (delta 16), reused 79 (delta 13), pack-reused 0\u001B[K\r\n",
      "Unpacking objects: 100% (82/82), done.\r\n",
      "mv: cannot move 'deepprior/data' to './data': Directory not empty\r\n",
      "mv: cannot move 'deepprior/models' to './models': Directory not empty\r\n",
      "mv: cannot move 'deepprior/utils' to './utils': Directory not empty\r\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "_uuid": "1219b3de-35e8-44a6-aecd-429316217e88",
    "_cell_guid": "e1c836f8-2cee-4ce6-88ef-59e325fa9019",
    "trusted": true
   },
   "cell_type": "markdown",
   "source": [
    "# Import libs"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "_uuid": "494f64a8-320b-4baf-9978-bf4eaadc0ee8",
    "_cell_guid": "4698cb99-1028-4a2d-9dd2-f2b85b7c4b59",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "from models.resnet import ResNet\n",
    "from models.unet import UNet\n",
    "from models.skip import skip\n",
    "import torch\n",
    "import torch.optim\n",
    "\n",
    "from utils.inpainting_utils import *\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark =True\n",
    "dtype = torch.cuda.FloatTensor\n",
    "\n",
    "PLOT = True\n",
    "imsize = -1\n",
    "dim_div_by = 64"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "2614514f-779f-4fec-85d4-9db6c0b640a9",
    "_cell_guid": "8facb06f-349b-4f6f-b0b4-18e9ad64da05",
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio.transforms as transformers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def reg_wav(wav, sr):\n",
    "    s = 3 * sr\n",
    "    if wav.shape[0] == 2:\n",
    "        wav = torch.mean(wav, dim=0, keepdim=True)\n",
    "    if wav.shape[1] < s:\n",
    "        pad_size = s - wav.shape[1]\n",
    "        wav = np.pad(wav, ((0, 0), (0, pad_size)), mode='wrap')\n",
    "        wav = torch.from_numpy(wav)\n",
    "    wav = wav[:, 0:s]\n",
    "    return wav\n",
    "\n",
    "\n",
    "to_spec = nn.Sequential(\n",
    "    transformers.MelSpectrogram(sample_rate=44100, n_fft=1022, hop_length=172,\n",
    "                                n_mels=256, f_min=20, f_max=8300, ),\n",
    "    transformers.AmplitudeToDB(top_db=80))\n",
    "\n",
    "path1 = '/kaggle/input/testff/test/street.wav'\n",
    "path2 = '/kaggle/input/testff/test/car.wav'\n",
    "wav1, sr1 = torchaudio.load(path1)\n",
    "wav2, sr2 = torchaudio.load(path2)\n",
    "wav1 = reg_wav(wav1, sr1)\n",
    "wav2 = reg_wav(wav2, sr2)\n",
    "mix = 0.1 * wav1 + 0.9 * wav2\n",
    "#torchaudio.save('data/test/car2.wav', wav1, sr1)\n",
    "#torchaudio.save('data/test/street2.wav', wav2, sr2)\n",
    "#torchaudio.save('data/test/mix.wav', mix, sr1)\n",
    "spec1 = to_spec(wav1)\n",
    "spec2 = to_spec(wav2)\n",
    "spec3 = to_spec(mix)\n",
    "spec1 = spec1.to(device)\n",
    "spec2 = spec2.to(device)\n",
    "mix = mix.to(device)"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/kaggle/input/testff/test/street.wav not found or is a directory",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-18-cb9e057a3476>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0mpath1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'/kaggle/input/testff/test/street.wav'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[0mpath2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'/kaggle/input/testff/test/car.wav'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 30\u001B[0;31m \u001B[0mwav1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msr1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorchaudio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     31\u001B[0m \u001B[0mwav2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msr2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorchaudio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[0mwav1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreg_wav\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwav1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msr1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/default/lib/python3.7/site-packages/torchaudio/__init__.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(filepath, out, normalization, channels_first, num_frames, offset, signalinfo, encodinginfo, filetype)\u001B[0m\n\u001B[1;32m     83\u001B[0m         \u001B[0msignalinfo\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msignalinfo\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     84\u001B[0m         \u001B[0mencodinginfo\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mencodinginfo\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 85\u001B[0;31m         \u001B[0mfiletype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfiletype\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     86\u001B[0m     )\n\u001B[1;32m     87\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/default/lib/python3.7/site-packages/torchaudio/_sox_backend.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(filepath, out, normalization, channels_first, num_frames, offset, signalinfo, encodinginfo, filetype)\u001B[0m\n\u001B[1;32m     23\u001B[0m     \u001B[0;31m# check if valid file\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misfile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mOSError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"{} not found or is a directory\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m     \u001B[0;31m# initialize output tensor\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOSError\u001B[0m: /kaggle/input/testff/test/street.wav not found or is a directory"
     ]
    }
   ]
  },
  {
   "metadata": {
    "_uuid": "c14b804e-0cbb-4338-816f-c65919c48570",
    "_cell_guid": "36851f50-3891-40ba-a03a-746b01214308",
    "trusted": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "# Choose figure"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "_uuid": "4caa90f4-c71f-4dbe-902e-31f6361b1946",
    "_cell_guid": "21e221a1-2c2e-4e54-aa32-9bd63872afd7",
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "## Fig 6\n",
    "# img_path  = 'data/inpainting/vase.png'\n",
    "# mask_path = 'data/inpainting/vase_mask.png'\n",
    "\n",
    "## Fig 8\n",
    "# img_path  = 'data/inpainting/library.png'\n",
    "# mask_path = 'data/inpainting/library_mask.png'\n",
    "\n",
    "## Fig 7 (top)\n",
    "img_path  = '/kaggle/working/data/inpainting/kate.png'\n",
    "mask_path = '/kaggle/working/data/inpainting/kate_mask.png'\n",
    "\n",
    "# Another text inpainting example\n",
    "# img_path  = 'data/inpainting/peppers.png'\n",
    "# mask_path = 'data/inpainting/peppers_mask.png'\n",
    "\n",
    "NET_TYPE = 'skip_depth6' # one of skip_depth4|skip_depth2|UNET|ResNet"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "f643295f-405b-4efb-9358-913235b76f03",
    "_cell_guid": "72f1b627-3f23-4a62-93bd-30abf62a48fa",
    "trusted": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "# Load mask"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "_uuid": "98b9ebbc-2178-4beb-83fc-d15808e1e4e6",
    "_cell_guid": "b6b0f585-166b-4836-aff0-702a304e5081",
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "img_pil, img_np = get_image(img_path, imsize)\n",
    "img_mask_pil, img_mask_np = get_image(mask_path, imsize)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "6ae34293-f656-48be-b6e1-ce92c7f9c298",
    "_cell_guid": "fa7c4cd1-b6a7-4008-a52a-2b4ae2ad49a6",
    "trusted": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Center crop"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "_uuid": "1ce9a11c-8cc5-4c68-95a5-e0047fa007cc",
    "_cell_guid": "4c83c8cd-5f71-4670-acb9-8243b6cd51f7",
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "img_mask_pil = crop_image(img_mask_pil, dim_div_by)\n",
    "img_pil      = crop_image(img_pil,      dim_div_by)\n",
    "\n",
    "img_np      = pil_to_np(img_pil)\n",
    "img_mask_np = pil_to_np(img_mask_pil)\n",
    "img_np = spec1.cpu().numpy()\n",
    "img_mask_np = spec2.cpu().numpy()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "a959311c-10f0-4ff7-b970-cc2981416564",
    "_cell_guid": "698900b5-ce23-452d-bbf5-0925b804602a",
    "trusted": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Visualize"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "_uuid": "47afd40a-6047-4de0-bcc8-604e7c1a38e9",
    "_cell_guid": "226b3114-f677-4a14-9a41-cf31482cfb35",
    "trusted": true,
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "img_mask_var = np_to_torch(img_mask_np).type(dtype)\n",
    "print(img_np.shape, img_mask_np.shape)\n",
    "plot_image_grid([img_np, img_mask_np, 0.5 * img_mask_np + 0.5 * img_np], 3,11);"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "03cea424-3f92-47cd-967c-aac789e555c9",
    "_cell_guid": "4bb28e0b-364d-495f-8b7f-4b898e2c8814",
    "trusted": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "_uuid": "07681c35-788f-454f-86f4-0dc4ecac7159",
    "_cell_guid": "6f65d241-4885-4c4a-b99b-84567436330c",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "pad = 'reflection' # 'zero'\n",
    "OPT_OVER = 'net'\n",
    "OPTIMIZER = 'adam'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "639cd504-8a30-4480-9b82-34c3b4693088",
    "_cell_guid": "58014a87-597d-403d-b639-b8dfc70336e5",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "if 'vase.png' in img_path:\n",
    "    INPUT = 'meshgrid'\n",
    "    input_depth = 2\n",
    "    LR = 0.01 \n",
    "    num_iter = 5001\n",
    "    param_noise = False\n",
    "    show_every = 50\n",
    "    figsize = 5\n",
    "    reg_noise_std = 0.03\n",
    "    \n",
    "    net = skip(input_depth, img_np.shape[0], \n",
    "               num_channels_down = [128] * 5,\n",
    "               num_channels_up   = [128] * 5,\n",
    "               num_channels_skip = [0] * 5,  \n",
    "               upsample_mode='nearest', filter_skip_size=1, filter_size_up=3, filter_size_down=3,\n",
    "               need_sigmoid=True, need_bias=True, pad=pad, act_fun='LeakyReLU').type(dtype)\n",
    "    \n",
    "elif ('kate.png' in img_path) or ('peppers.png' in img_path):\n",
    "    # Same params and net as in super-resolution and denoising\n",
    "    INPUT = 'noise'\n",
    "    input_depth = 32\n",
    "    LR = 0.01 \n",
    "    num_iter = 6001\n",
    "    param_noise = False\n",
    "    show_every = 50\n",
    "    figsize = 5\n",
    "    reg_noise_std = 0.03\n",
    "    \n",
    "    net = skip(input_depth, img_np.shape[0], \n",
    "               num_channels_down = [128] * 5,\n",
    "               num_channels_up =   [128] * 5,\n",
    "               num_channels_skip =    [128] * 5,  \n",
    "               filter_size_up = 3, filter_size_down = 3, \n",
    "               upsample_mode='nearest', filter_skip_size=1,\n",
    "               need_sigmoid=True, need_bias=True, pad=pad, act_fun='LeakyReLU').type(dtype)\n",
    "    \n",
    "elif 'library.png' in img_path:\n",
    "    \n",
    "    INPUT = 'noise'\n",
    "    input_depth = 1\n",
    "    \n",
    "    num_iter = 3001\n",
    "    show_every = 50\n",
    "    figsize = 8\n",
    "    reg_noise_std = 0.00\n",
    "    param_noise = True\n",
    "    \n",
    "    if 'skip' in NET_TYPE:\n",
    "        \n",
    "        depth = int(NET_TYPE[-1])\n",
    "        net = skip(input_depth, img_np.shape[0], \n",
    "               num_channels_down = [16, 32, 64, 128, 128, 128][:depth],\n",
    "               num_channels_up =   [16, 32, 64, 128, 128, 128][:depth],\n",
    "               num_channels_skip =    [0, 0, 0, 0, 0, 0][:depth],  \n",
    "               filter_size_up = 3,filter_size_down = 5,  filter_skip_size=1,\n",
    "               upsample_mode='nearest', # downsample_mode='avg',\n",
    "               need1x1_up=False,\n",
    "               need_sigmoid=True, need_bias=True, pad=pad, act_fun='LeakyReLU').type(dtype)\n",
    "        \n",
    "        LR = 0.01 \n",
    "        \n",
    "    elif NET_TYPE == 'UNET':\n",
    "        \n",
    "        net = UNet(num_input_channels=input_depth, num_output_channels=3, \n",
    "                   feature_scale=8, more_layers=1, \n",
    "                   concat_x=False, upsample_mode='deconv', \n",
    "                   pad='zero', norm_layer=torch.nn.InstanceNorm2d, need_sigmoid=True, need_bias=True)\n",
    "        \n",
    "        LR = 0.001\n",
    "        param_noise = False\n",
    "        \n",
    "    elif NET_TYPE == 'ResNet':\n",
    "        \n",
    "        net = ResNet(input_depth, img_np.shape[0], 8, 32, need_sigmoid=True, act_fun='LeakyReLU')\n",
    "        \n",
    "        LR = 0.001\n",
    "        param_noise = False\n",
    "        \n",
    "    else:\n",
    "        assert False\n",
    "else:\n",
    "    assert False\n",
    "\n",
    "net = net.type(dtype)\n",
    "net_input = get_noise(input_depth, INPUT, img_np.shape[1:]).type(dtype)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "a15e891b-8592-4339-84c4-2b7f8972084f",
    "_cell_guid": "45aa3254-8e8f-4d39-b126-5690a73c67d6",
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute number of parameters\n",
    "s  = sum(np.prod(list(p.size())) for p in net.parameters())\n",
    "print ('Number of params: %d' % s)\n",
    "\n",
    "# Loss\n",
    "mse = torch.nn.MSELoss().type(dtype)\n",
    "\n",
    "img_var = np_to_torch(img_np).type(dtype)\n",
    "mask_var = np_to_torch(img_mask_np).type(dtype)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "88afa77a-f0d1-42ad-8095-15a5900c2f09",
    "_cell_guid": "2f374f28-6636-4239-9989-4284ba808e06",
    "trusted": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "# Main loop"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def set_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def lr_decay(optimizer, epoch):\n",
    "    c = 500\n",
    "    if epoch % c == 0:\n",
    "        new_lr = LR / (10 ** (epoch // c))\n",
    "        optimizer = set_lr(optimizer, new_lr)\n",
    "        print('change learning rate to:{}'.format(new_lr))\n",
    "    return optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 0\n",
    "def closure():\n",
    "    \n",
    "    global i\n",
    "    \n",
    "    if param_noise:\n",
    "        for n in [x for x in net.parameters() if len(x.size()) == 4]:\n",
    "            n = n + n.detach().clone().normal_() * n.std() / 50\n",
    "    \n",
    "    net_input = net_input_saved\n",
    "    if reg_noise_std > 0:\n",
    "        net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
    "        \n",
    "        \n",
    "    out = net(net_input)\n",
    "   \n",
    "    total_loss = mse(out * mask_var, 0.5 * img_var + 0.5 * mask_var)\n",
    "    total_loss.backward()\n",
    "        \n",
    "    print ('Iteration %05d    Loss %f' % (i, total_loss.item()), '\\r', end='')\n",
    "    if  PLOT and i % show_every == 0:\n",
    "        out_np = torch_to_np(out)\n",
    "        plot_image_grid([out_np], factor=figsize, nrow=1)\n",
    "        \n",
    "    i += 1\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "net_input_saved = net_input.detach().clone()\n",
    "noise = net_input.detach().clone()\n",
    "\n",
    "p = get_params(OPT_OVER, net, net_input)\n",
    "optimize(OPTIMIZER, p, closure, LR, num_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {
    "_uuid": "b2513f51-55cb-471b-bc9a-5bc7b1f44e48",
    "_cell_guid": "65e22468-676a-4b92-a721-eb9128b84868",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "out_np = torch_to_np(net(net_input))\n",
    "plot_image_grid([out_np], factor=5);"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "6cdd3bea-eea8-424a-91ac-969e517b48e0",
    "_cell_guid": "0d553530-f6bb-4007-8d59-d148a4443fa2",
    "trusted": true,
    "scrolled": true
   },
   "cell_type": "code",
   "source": "i = 0\ndef closure():\n    \n    global i\n    \n    if param_noise:\n        for n in [x for x in net.parameters() if len(x.size()) == 4]:\n            n = n + n.detach().clone().normal_() * n.std() / 50\n    \n    net_input = net_input_saved\n    if reg_noise_std > 0:\n        net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n        \n        \n    out = net(net_input)\n   \n    total_loss = mse(out * mask_var, 0.5 * img_var + 0.5 * mask_var)\n    total_loss.backward()\n        \n    print ('Iteration %05d    Loss %f' % (i, total_loss.item()), '\\r', end='')\n    if  PLOT and i % show_every == 0:\n        out_np = torch_to_np(out)\n        plot_image_grid([out_np], factor=figsize, nrow=1)\n        \n    i += 1\n\n    return total_loss\n\nnet_input_saved = net_input.detach().clone()\nnoise = net_input.detach().clone()\n\np = get_params(OPT_OVER, net, net_input)\noptimize(OPTIMIZER, p, closure, LR, num_iter)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "4bcc3f2d-96c5-4a68-bdf7-e5fca2c540f9",
    "_cell_guid": "65c7da58-055c-424d-bb5d-d763187e757b",
    "trusted": true
   },
   "cell_type": "code",
   "source": "out_np = torch_to_np(net(net_input))\nplot_image_grid([out_np], factor=5);",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}